\chapter{Related Work} % Main chapter title

\label{ch:background} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}
This work extends earlier work in wearable AR, AR annotation, social proximity, virtual avatars, research collaboration, telepresence and sharing social experiences. This section examines previous related work on wearable AR, AR collaboration, and sharing social experiences from the social AR perspective. 

\section{Wearable AR} 

AR devices can be categorised into wearable (e.g., head-mounted displays, helmet or contact lens), handheld (e.g., phone, tablet) or projected displays where AR is projected onto a larger area regardless of where the user is looking~\cite{Peddie2017}. 

\cite{Feiner1997a} presented the first mobile wearable AR system in 1997 called "Touring Machine" combining a head-mounted display (HMD), hand-held tablet, and a backpack carrying a computer GPS and radio for wireless access. \cite{Hollerer1999} followed up and explored different user interfaces on a wearable see-through display. The interface allowed users to sketch pathways and annotate their world for collaborative AR systems. 

Wearable AR has added an extra dimension to AR allowing people to collaborate hands-free. \cite{Feiner1999} talked about what impact wearable computing (and being mobile in general) has from the social perspective. These implications include personal privacy concerns, connectivity, collaboration and how it looks and feels. 

Wearable AR has been explored for social interactions. For instance, \cite{Cheok2002a} developed a wearable mixed reality experience where social interactions take place. \cite{Amores2015} used gestures to communicate social interactions via wearable devices. \cite{Lee2019} used live 360 cameras to communicate between worker and helper. \cite{Shu2018} developed a system of facial recognition to facilitate social interactions. 

With wearable AR devices becoming affordable, available and ubiquitous, there is a need to understand design considerations for this new platform. Previous research has looked into using AR headsets for collaborative use. For example in enhancing face to face \cite{Billinghurst2002} or remote collaboration \cite{Gupta2016}. The research presented here explores the use of AR headsets for social interaction and shared experiences. Social interactions can be extrapolated from current social network interactions where \enquote{friends} share content and interact with another's content (i.e., placing likes and comments).

% There is a need to understand design considerations for this new platform. Previous research has looked into using wearable AR headsets for collaborative use, for example in enhancing face to face \cite{Billinghurst2002} or remote collaboration \cite{gupta2016you}. The research presented here explores the use of AR headsets for social interaction and shared experiences. 

\section{AR Annotation, Collaboration and Telepresence}

There are a number of examples of AR annotation demonstrations on mobile devices. For example, mobile AR browsers (e.g., Wikitude\footnote{https://www.wikitude.com/} or Junaio\footnote{https://en.wikipedia.org/wiki/Junaio}) can overlay AR tags in the real world using GPS and other motions sensors. While they were successful in demonstrating the concept of visualising AR annotation, the registration of virtual objects in the real world can be inaccurate and they can only be used in outdoor, large-scale environments. Mobile AR browsers usually create AR tags in advance, but recent research projects have investigated in-situ and interactive creation of AR tags. For example, \cite{Kim:2011:IAS} presented an interactive method where the user stands in a fixed position to calibrate the room model with the gyroscope data. The user can then touch and annotate locations with a rectangle where virtual content, like text, images and 3D models, can be overlaid. \cite{Langlotz:2012:OCP} developed a vision-based orientation tracking system to locate and visualise annotations with pixel accuracy on a real-time generated panorama. These systems assume pure rotation from the devices while the user has to stand at the same position while doing the annotation, which dramatically degrades the quality of the user experience.

A variety of AR annotation methods with wearable interfaces have also been presented. Sixsense \cite{Mistry:2009:WWU} used a wearable gestural interface for AR annotation. It consists of a camera and a small projector mounted on a hat or in a pendant. The camera tracks user hand gestures and the projector visually augments virtual content onto the physical objects with which the user is interacting. However, the system requires planar surfaces in front of the user for accurate projection because of the lack of a depth sensor. OmniTouch \cite{Harrison:2011:OTW} is a wearable projection system equipped with depth-sensing technology that enables interactive multi-touch applications on different surfaces. Both the depth camera and projector are mounted onto a form-fitting metal frame, which is worn on the shoulders, and secured with a chest strap. This system extends the typical scenarios supported by Sixsense to on-body surfaces or objects held in the hand for image projection. However, the system was still bulky and inconvenient to use because it needed to be connected to a desktop computer.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/Huang2013.PNG}
    \caption{HandsInAir: A Wearable System for Remote Collaboration}
    \label{fig:HandsInAir}
\end{figure}

Camera-equipped mobile devices provide a quick way of capturing and sharing experiences and spaces. Wearable computers that combine HMDs and cameras provide new opportunities for collaboration. For example, the Google Glass\footnote{http://www.google.com/glass/} wearable system has a camera, mic, and head-worn display.

There has been a significant amount of earlier research on remote collaboration using head-mounted cameras and displays. For example, allowing a remote user to place virtual annotations on the live camera view from a head-worn camera and showing the result in the wearers HMD, has been shown to enhance remote collaboration \cite{Fussell2003}. Other systems allow a remote user to place their hands in the local user's view \cite{Huang2013} (Figure \ref{fig:HandsInAir}). 

In many wearable and mobile AR applications, remote collaboration is the main purpose for sharing a view of the user's world. For example, remote expert collaboration systems have been developed where a local worker with an AR display can share a live video view of their workspace with a remote expert \cite{Billinghurst2002}. The remote expert can provide visual feedback with AR graphical cues.  However, most of these systems have just been developed for collaboration between small numbers of users, and not for more extensive social networks. (Figure \ref{fig:Billinghurst2002})

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/Billinghurst2002.PNG}
    \caption{Live virtual video avatars for remote collaborative AR interfaces. \cite{Billinghurst2002}}
    \label{fig:Billinghurst2002}
\end{figure}

When people connect in this way, they may also want to share different amounts of information about their surroundings with each other. For example, users who are close friends in a social network may be happy to share a 3D virtual view of their surroundings and have the remote user appear as an AR avatar in their real space, while those that are strangers may only want to have an audio connection and not show anything of their surroundings to preserve their privacy \cite{Oetzel2011}. The position on the social continuum could be used to modify how much information a person can share about themselves and their surroundings.

\cite{Fuchs2014} (Figure \ref{fig:Fuchs2014}) studied telepresence via a scanned 3D environment to enable social connections with people and simulated face-to-face interactions. The remote person was scanned and reconstructed live in the local environment. They forecasted that 3D telepresence was going to be more popular when technology becomes more capable.

\begin{figure}
    \centering
    \includegraphics[width=.8\linewidth]{images/Fuchs2014.PNG}
    \caption{Real-time 3D reconstruction of human \cite{Fuchs2014}}
    \label{fig:Fuchs2014}
\end{figure}

\section{Social Proximity \& Virtual Avatars}

In the social AR/VR space, previous work implemented a variety of visual representations of self and others. \cite{Fanello2016} prototyped live sharing of a full scan of a person's body with remote users using 3D cameras and the HoloLens\footnote{https://www.microsoft.com/en-nz/hololens}. For representing “people” in AR space, \cite{Sousa2016} (Figure \ref{fig:Sousa2016}) studied the concept of \enquote{personal space} and \enquote{social bubbles} in terms of proxemic interactions between people in different places. They used floor projections and hand-held devices to communicate the presence of remote people. They also established a \enquote{gradual engagement model for remote proxemics} based on distance from the user which consisted of 1) personal, 2) engaged, 3) peripheral and 4) ambient.

\begin{figure}
    \centering
    \includegraphics[width=.8\linewidth]{images/Sousa2016.PNG}
    \caption{Gradual engagement model for remote proxemics. \cite{Sousa2016}}
    \label{fig:Sousa2016}
\end{figure}

Most previous work has focused on how visual representation and proximity could be used to organise an AR representation of a person's social network. However, this information could also be used to modify the contextual information being shared by a user out to their social network as well. 
% For example, a person may not want strangers to know what they look like, and so would prefer being represented as a stylistic icon to people that do not know them, but would be more comfortable sharing a lifelike representation to those that are close to them. 

Similarly, some companies (such as High Fidelity\footnote{https://highfidelity.com/}, Sansar\footnote{https://www.sansar.com/}, Itsme3D\footnote{https://www.itsme3d.com/}) and other VR shared worlds) are building social VR experiences in which users are represented as 3D virtual avatars. In the social VR space, previous work implemented a variety of visual representations of self and others. For instance, virtual avatars have been used to share social experiences such as in Facebook Spaces\footnote{https://www.facebook.com/spaces} (Figure \ref{fig:facebook-spaces}, left) where users can meet in VR, take selfies and teleport to a 360-degree video. Virtual avatars for self and others are represented as floating face and upper body rendered in a VR background. 

\begin{figure}
    \centering
    \includegraphics[width=.4\linewidth]{images/facebook-spaces.jpg}
    \includegraphics[width=.4\linewidth]{images/altspace-vr.png}
    \caption{Examples of avatars in VR. Facebook Spaces (left) and AltSpaceVR (right)}
    \label{fig:facebook-spaces}
\end{figure}

As for the social AR spaces, there have been limited examples of social applications. Avatar Chat was introduced by Magic Leap\footnote{https://www.magicleap.com/experiences/social} (Figure \ref{fig:ml-avatar-chat}) in late 2018. The app allows users to connect with their social contacts and view their avatar overlaid on top of their physical environment. Users can share emojis, connect to a group "avatar" chat and talk about images overlaid in their space. The avatars are cartoonish-looking representing the upper half of the body floating in the AR space. Natural hand gestures are detected using the depth cameras on the device, assisted by computer vision algorithms and translated to a pre-defined set of virtual hand gestures to the other participants in the chat session. 

Saptiate\footnote{http://spatiate.com/} is another social app launched in early 2019 on Magic Leap where users can connect in a chat session with primitive avatars, sketch in their AR world, and share the sketching in real time with connected users. The users can see each other's avatars and hear their voices. 

\begin{figure}
    \centering
    \includegraphics[width=.4\linewidth]{images/avatar-chat-1.png}
    \includegraphics[width=.4\linewidth]{images/avatar-chat-2.png}
    \caption{Example of avatars in AR - Avatar Chat by Magic Leap}
    \label{fig:ml-avatar-chat}
\end{figure}

However, representing social contacts in VR/AR can be cumbersome. It will be more cluttered and overwhelming to represent the data content that social avatars are trying to share. Representing social data has been imagined by futuristic concept videos (e.g., "Hyper Reality"\footnote{https://www.youtube.com/watch?v=YJg02ivYzSs} and "Merger"\footnote{https://www.youtube.com/watch?v=SqW2dEkiD-Y}) where social data can clutter the AR view of the user. It will also raise privacy and ethical concerns.

\cite{Jo2016} studied the influence on co-presence of the background environment (AR vs VR) and the fidelity of the avatar representation of the remote user (photo-realistic vs pre-built). They found that more realistic avatars had a positive impact on the feeling of co-presence between remote collaborators. \cite{Volante2016} also studied the impact of the visual appearance of avatars (realistic vs. stylized) on the inter-personal emotional response of participants. They also found that more visual realism has lower negative affects on the PANAS scale, which measure the intensity of the emotion at a given time. 
Researchers have been investigating social aspects of multi-user VR environments. \cite{Ducheneaut2006} studied massive multiplier online games in terms of social activities, and found that while users may prefer to be with other players, they do not necessarily like actively interacting with them. This led us to think that users may want to have the sense of the presence of social contacts around them, but not necessarily interact with them.

\cite{Harris2009} studied the social behaviour of users of Second Life, and found that users became less active over time and go to familiar places rather than being explorative and actively teleporting/flying. This suggests that people prefer routine and to be surrounded by familiar faces/places over time, forming a social group.

% mark: [you should about how they handle crowds or if everyone is just represented the same] 
However, there has been very little research into social representation in AR. The AR space is more challenging in terms of finding the best locations to fit avatars in the real world so they don't interfere with physical objects or appear suspended in mid-air. However, a social AR application can also allow people to see their friends while doing other tasks; users do not have to use an immersive VR environment to see their social contacts.

Researchers have also explored different ways of managing a large amount of information tags in AR interfaces. \cite{Julier2002} showed how environmental cues, such as distance and user context, can be used to filter AR content into the most relevant information. \cite{Hollerer2001} describe how view management techniques can be used to ensure that virtual objects can be easily seen in collaborative AR interfaces. Similarly \cite{Grasset2012} show how an image-based approach can be used to ensure AR information tags don't overlap in handheld AR. 

This previous research shows that visual fidelity can be used to distinguish between virtual avatars. Different visual representations and spatial cues can also be used to distinguish between information tags in an AR interface. However, there has been little or no research on how to manage social network representations in AR for large numbers of connections. In the next section, we show how visual and proximity cues could be used to organise contacts in a wearable social AR interface.

% \todo[inline]{[In this section you should probably also mention how VR Virtual Avatar environments handle hundreds or more simultaneous users. For example, what tools does second life have for connecting people to hundreds of others. This is directly relevant to your thesis topic]}

\section{Sharing Social Experiences \& Social Networks}

Advancements in mobile phone hardware and increased network connectivity made live video streaming apps popular among smartphone users. Live video streaming apps have been used for sharing social experiences in various contexts. For instance, a person attending a conference or a concert could use her mobile phone to stream the event to her friends and family who could not be there. Similarly, live video streaming apps have also been used for social journalism, turning laypersons into live reporters. Consequently, these apps are now available from different sources with applications such as Periscope\footnote{https://www.periscope.tv/} and Facebook Live\footnote{https://live.fb.com/} among the most popular. These apps allow the users who are sharing to receive comments on the video they are sharing, and to receive simple graphical feedback. 

Common features, such as using the phones' camera which can be either pointed outward (recording what the user sees) or inward (where the user appears in the video), allow users to send a live video stream of what they are doing to hundreds or even thousands of viewers. The purpose of sharing the video is social, so the experience is improved if the viewer can also provide feedback.

\begin{figure}
    \centering
    \includegraphics[width=.4\linewidth]{images/periscope.png}
    \includegraphics[width=.4\linewidth]{images/facebook-live.png}
    \caption{Examples of live-streaming apps}
    \label{fig:live-streaming}
\end{figure}

In these applications, the feedback comments usually appear in a list below or beside the video being shared (Figure \ref{fig:live-streaming}), separate from the visual context of what the viewer is commenting on. This may cause problems when the person sending the video changes his or her viewpoint. For example, a viewer might send the comment “I really like that view”, but by the time the comment appears, the view might already have changed from the view being commented on.

Future social interactions with wearable AR can be extrapolated from current social network interactions where friends share content and interact with others' content on mobile platforms such as Facebook and Instagram. One trend with mobile social networks is live streaming of a view of a user’s surroundings. For example, Facebook live allows a person with a mobile phone to live stream to remote collaborators. Similarly, wearable AR systems have already been developed that enable people to share a view of their surroundings. For example, the Shared Sphere work of \cite{lee2017mixed} allows a user with a wearable AR display to live stream a 360-degree video of their surroundings to a remote collaborator, although only between pairs of users. 

It is easy to imagine that in the future it will be possible for wearable AR systems to be used to capture and share a 3D view of the user's surroundings with hundreds or thousands of followers on a social network. However, before this becomes commonplace, many important research questions need to be addressed. For example, would a person be comfortable sharing a view of their surrounding real space with relative strangers? This work aims to explore how wearable AR systems could share a user’s surrounding room environment with social contacts and to measure how comfortable the sharer and the viewer would feel regarding privacy within different interface options. 

If AR is to be used to represent contacts in social networks, there could be a large number of contacts to show. Our research has benefited from earlier work on different ways of managing large amounts of information in AR interfaces.


%----------------------------------------------------------------------------------------
%	Summary
%----------------------------------------------------------------------------------------
\section{Summary}

This work aims to layout the space of the AR continuum for social sharing experiences by looking at parameters and options that can be changed in terms of people, objects and the environment to create a shared AR experience. 

Building on previous work on proximity-based relationships \cite{Sousa2016}, we focus on the shared contents of social avatars in an asynchronous situation. Unlike previous work on social avatars, we study representing social contacts in a large-scale network. We aim to reduce the clutter that may be caused by displaying the social avatars and their shared content. We address the question of how we can use the social relationship between avatars and the viewer as a way to filter and enhance viewing the shared-content experiences. 

The scope of this thesis is to explore options of visual user experience design in social AR including displaying contacts, displaying shared data, and displaying shared environments. This thesis does not necessarily cover all possible experiences, but highlights the main points of interactions and reports on user studies measuring the sense of presence and privacy concerns that may occur from these experiences. 

The summary of the trends in the past work is that
% summary of wearable AR 
wearable AR has been the target of many previous developments and research directions. Aiming to enable true outdoor experiences and to be untethered to physical places would allow more exploration of the surrounding world. 
% summary of AR collaboration 
Most of the previous work on AR focused on AR applications for remote collaboration and expert-helper situations. Very few focused on how do we use AR in connecting with our friends and family and for sharing social experiences. Most current commercial applications are focused on expert-helper situations.
% Summary of social AR
In the social aspects of AR, previous work showed few attempts of representing social networks in AR and VR. Also, some applications looked into scanning and using human avatar representations for social connections with others.

The limitation of past work is that it was mainly focused on the remote collaboration of local worker/remote helper situations. However, it did not address sharing social experiences with friends and family scenarios. There is some new work in this area, but not enough to cover all dimensions in this field. The gap that this thesis is addressing is the user interaction design of future wearable AR interfaces so that it can be easily used for sharing social experiences with family and friends. 

This thesis is important because it explores different ways of presenting social networks on wearable AR and helps application designers and developers by providing insights into building similar applications that have a higher chance of being effective and acceptable to users. 

The contributions to the current state-of-the-art are: 1) building software prototypes of sharing social experiences on wearable AR platforms, 2) Running user studies on these prototypes and analysing the results, 3) providing the foundations of the design space of sharing social experiences on wearable AR. 

% \todo[inline]{Gun: You should have a paragraph summarizing the trend in the past work and what is the limitation to support why your work is important. Or you may add a section at the end of this chapter that does an overall review of the whole chapter and describe how your thesis makes contribution to the current state-of-the-art.}

% \todo[inline]{You should end the chapter with a summary of the research gap that you’re going to be addressing.}

% \todo[inline]{[At the end of the related work section you should have a sub-section that provides a summary of the research, particularly highlighting the research gaps and shortcomings of this earlier work. You can then talk about which aspects that you are going to explore further in your PhD thesis. Highlight what is the novelty of your thesis, and what will be the main contributions that your research will make, relative to the earlier related work] }




% \subsection{Sharing for Collaboration}
% \subsection{Sharing for Social}
