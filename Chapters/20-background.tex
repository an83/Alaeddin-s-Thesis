\chapter{Related Work} % Main chapter title

\label{ch:background} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

In this chapter, in Section 2.1 we first begin with a review of previous work regarding basic wearable AR platforms and various user interactions. In Section 2.2 we then outline recent research in remote collaboration and sharing experiences using AR platform, and in Section 2.3 summarize the main tasks that require further investigation in the area of sharing social experiences on wearable AR devices.

\section{Wearable AR}

With wearable Augmented Reality (AR) devices becoming affordable, available and ubiquitous, there is a need to understand design considerations for this new platform. Previous research has looked into using AR headsets for collaborative use. The research presented here explores the use of AR headsets for social interaction and shared experiences. Social interactions can be extrapolated from current social network interactions where \enquote{friends} share content and interact with another's content (i.e., likes, comments).

There is a need to understand design considerations for this new platform. Previous research has looked into using wearable AR headsets for collaborative use, for example in enhancing face to face \cite{Billinghurst2002} or remote collaboration \cite{gupta2016you}. The research presented here explores the use of AR headsets for social interaction and shared experiences. 

\section{AR Annotation}

There have been a number of examples of AR annotation demonstration on mobile devices. For example, mobile AR browsers (e.g. Wikitude or Junaio) can overlay AR tags in the real world using  GPS and other motions sensors. While they were successful with demonstrating the concept of visualizing AR annotation, the registration of virtual objects in the real world can be inaccurate and they can only be used in outdoor large-scale environments. Mobile AR browsers usually create AR tags in advance, but recent research projects have investigated in-situ and interactive creation of AR tags. Kim et al. \cite{Kim:2011:IAS} presented an interactive method where the user stands in a fixed position to calibrate the room model with the gyroscope data. The user can then touch and annotate locations with a rectangle where virtual content, like text, image and 3D models, can be overlaid.  Langlotz et al. \cite{Langlotz:2012:OCP} developed a vision-based orientation tracking system to locate and visualize annotations with pixel accuracy on a real-time generated panorama. Both these systems assume pure rotation from the devices while the user has to stand at the same position while doing the annotation, which dramatically reduces the user experience.

Meanwhile, a variety of AR annotation methods with wearable interfaces have been presented as well. Sixsense \cite{Mistry:2009:WWU} used a wearable gestural interface for AR annotation. It consists of a camera and a small projector mounted on a hat or coupled in a pendant. The camera tracks user hand gestures and the projector visually augments virtual content on the physical objects that the user is interacting with. However it requires planar surfaces in front of the user for accurate projection because of the lacking of a depth sensor. OmniTouch \cite{Harrison:2011:OTW} is a wearable projection system equipped with depth-sensing technology that enables interactive multi-touch applications on different surfaces. Both the depth camera and projector are rigidly mounted to a form-fitting metal frame, which is worn on the shoulders, and secured with a chest strap. This system extended the typical scenarios supported by Sixsense to  users' on-body surfaces or objects held in hands for image projection by using depth sensors. However, the system itself is still bulky and inconvenient to use because of the need to connect to a desktop computer.

Camera equipped mobile devices provide a quick way of capturing and sharing experiences and spaces. Wearable
computers that combine head mounted displays (HMDs) and cameras provide new opportunities for collaboration. For example, Glass\footnote{http://www.google.com/glass/} has a camera, mic, and head worn display.

There has been a significant amount of earlier research on remote collaboration using head mounted cameras and displays. For example, allowing a remote user to place virtual annotations on the live camera view from a head worn camera and show the result in the wearers HMD, enhancing remote collaboration \cite{Fussell2003}. Other systems allow a remote user to place their hands in the local users view \cite{Huang2013}. However, in most of these systems the remote user's view is limited to the live feed from the head worn camera reducing awareness of the user's surroundings. In our research we are interested in how wearable users could rapidly capture and share their surroundings using panorama imagery. Figure \ref{fig:ismar14:concept} shows a concept of how a wearable user could pan their head around and capture their surroundings and then share them with a remote collaborator as an immersive panorama. Once shared, both users could view the space independently, talk to each other, and point or draw on the image to easily communicate about the scene. Panoramic imagery can provide an increased understanding of the user's space. In this poster we report on progress towards this vision of Social Panoramas. This extends the earlier panorama work of Cheng \cite{L.Cheng1998} by including HMD and tablet interaction, and focusing on Social Presence.

\section{Social Proximity}

In the social AR/VR space, previous work implemented visual representations of self and others (e.g., Facebook Spaces\footnote{https://www.facebook.com/spaces}) to create fun overlays and representations of the face and upper body. In other research, \cite{Fanello2016} prototyped live sharing of a full scan of a person's body with remote users using 3D cameras and the HoloLens\footnote{https://www.microsoft.com/en-nz/hololens}. For representing “people” in AR space, \cite{Sousa2016} studied the concept of \enquote{personal space} and \enquote{social bubbles} in terms of proxemic interactions between people in different places. They used floor projections and hand-held devices to communicate the presence of remote people. They also established a \enquote{gradual engagement model for remote proxemic} based on distance from the user which consists of 1) personal, 2) engaged, 3) peripheral, and 4) ambient.

Most work has focused on how visual representation and proximity could be used to organize an AR representation of a person's social network. However, this information could also be used to modify the contextual information being shared by a user out to their social network as well. For example, a person may not want strangers to know what they look like, and so would prefer being represented as a stylistic icon to people that do not know them, but would be more comfortable sharing a lifelike representation to those that are close to them. 

Building on previous work in proximity-based relationship \cite{Sousa2016}, we focus on the shared contents of social avatars in asynchronous situation. Unlike previous work in social avatars, we study representing social contacts in large-scale network. We aim to reduce clutter that maybe caused by displaying the social avatars and their shared content. We address the question of how we can use social relationship between avatars and the viewer as a way to filter and enhance viewing the shared-content experience. 


\section{Virtual Avatars}

Virtual avatars have been studied in social applications such as social networks and multi-users environments such as in Second Life \cite{Kaplan2009} and other VR games. Virtual avatars have been used to share social experiences such as in Facebook Spaces\cite{Facebook} where users can meet in VR, take selfies and teleport to a 360 video. However, representing social contact in VR/AR can be overwhelming \cite{Nassani2017b}. It will be more overwhelming to represent the data content that social avatars are trying to share. 

\cite{Jo2016} studied the influence on co-presence of the background environment (AR vs VR) and the fidelity of the avatar representation of the remote user (photo-realistic vs pre-built). They found that more realistic avatars had a positive impact on the feeling of co-presence between remote collaborators. \cite{Volante2016} also studied the impact of the visual appearance of avatars (realistic vs. stylized) on the inter-personal emotional response of participants. They also found that more visual realism has lower negative effects.

Researchers  have been investigating social aspects of multi-user VR environment. \cite{Ducheneaut2006} studied massive multiplier online games in terms of social activities, and  found that while users may prefer to be with other players, they don't necessarily like actively interacting with them. This led us to thinking that users may want to have the sense of the presence of social contacts around them, but not necessarily interact with them.

\cite{Harris2009} studied the social behaviour of users of Second Life, and they found that users become less active over time and go to familiar places rather than being explorative and actively teleporting/flying. This suggests that people prefer routine and to be surrounded by familiar faces/places over time, forming social group.

Some companies (such as High Fidelity\footnote{https://highfidelity.com/} and Itsme3D\footnote{https://www.itsme3d.com/}) are building social VR experiences in which people are represented as 3D virtual avatars.
% mark: [you should about how they handle crowds or if everyone is just represented the same] 
However, there has been very little research into social representation in AR. The AR space is more challenging in terms of finding the best locations to fit avatars in the real world so they don't interfere with physical objects or appear suspended in mid-air. However, a social  AR application can also allow people to see their friends while doing other tasks; users don't have into switch to an immersive VR environment to see their social contacts.

Researchers have also explored different ways of managing a large amount of information tags in AR interfaces.  Julier et al. showed how environmental cues such as distance, and user context can be used to filter AR content into the most relevant information \cite{Julier2002}. Hollerer et al. describe how view management techniques can be used to ensure that virtual objects can be easily seen in collaborative AR interfaces \cite{Hollerer2001}. Similarly Grasset et al. show how an image-based approach can be used to ensure AR information tags don't overlap in handheld AR \cite{Grasset2012}. 

This previous research shows that visual fidelity can be used to distinguish between virtual avatars. Different visual representations and spatial cues can also be used to distinguish between information tags in an AR interface. However there has been little or no research on how to manage social network representations in AR for large numbers of connections. In the next section, we show how visual and proximity cues could be used to organize contacts in a wearable social AR interface.

\section{Remote Collaboration}

In many mobile AR applications, the technology is used to share a view of the user's world. For example, remote expert collaboration systems have been developed where a local worker with an AR display can share a live video view of their workspace with a remote expert \cite{Billinghurst2002}. The remote expert can provide visual feedback with AR graphical cues.

When people connect in this way, they may also want to share different amounts of information about their surroundings with each other. For example, users who are close in a social network may be happy to share a 3D virtual view of their surroundings and have the remote user appear as an AR avatar in their real space, while those that are strangers may only want to have an audio connection and not show anything of their surroundings to preserve their privacy but allow them to share and control their privacy \cite{Oetzel2011}. The position on the social continuum could be used to modify how much information a person can share about themselves and their surroundings.

If AR is to be used to represent contacts in social networks, there could be a large number of contact to show. So our research could also benefit from earlier work on different ways of managing large amounts of information in AR interfaces. \cite{Julier2002} showed how environmental cues such as distance, and user context could be used to filter AR content into the most relevant information. View management techniques can be used to ensure that virtual objects can be easily seen in collaborative AR interfaces \cite{Hollerer2001}. Similarly, an image-based can be used to ensure AR information tags do not overlap in the AR view \cite{Grasset2012}. 

In some mobile AR applications, the technology can be used to share a view of the user's world. For example, remote collaboration systems have been developed where a local worker with an AR display can share a live video view of their working space with a remote expert \cite{Billinghurst2002}, and the remote expert can provide visual feedback with AR graphical cues. However, most of these systems have just been developed for collaboration between small numbers of users, and not for more extensive social networks.

\section{Telepresence}

\cite{Fuchs2014} studied telepresence via a scanned 3D environment to enable social connections with people and simulated face-to-face interactions. The remote person was scanned and reconstructed live in the local environment. They forecast that 3D telepresence is going to be more popular when technology is more capable. Similarly, some companies (such as High Fidelity and Itsme3D) are building social VR experiences in which users are represented as 3D virtual avatars.


Although there has been considerable research into social representation in VR, there has been very little research in AR. There are some challenges with AR, such as finding the best locations to fit virtual avatars in the real world, so they do not interfere with physical objects or appear suspended in mid-air. However, a social AR application can also allow people to see their friends while doing other tasks; users do not have to switch to an immersive VR environment to see their social contacts.

\section{Sharing Social Experiences}

Advancements in mobile phone hardware and increased network connectivity made live video streaming apps popular among smart phone users. Live video streaming apps have been used for sharing social experiences in various contexts. For instance, a person attending a conference or a concert could use her mobile phone to stream the event to her friends and family who could not be there. Similarly, live video streaming apps have also been used for Social journalism turning laypersons into live reporters. Consequently, these apps are now available from different sources with applications such as Periscope\footnote{https://www.periscope.tv/} and Facebook Live\footnote{https://live.fb.com/} among the most popular apps. 

They all share common features such as using the phones' camera which can be either pointed outward (recording what the user sees) or inward (where the user appears in the video) and allowing users to send a live video stream of what they are doing to hundreds or even thousands of viewers. The purpose of sharing the video is social, so the experience is improved if the viewer can also provide feedback. Applications like Periscope allow the users who are sharing to receive comments on the video they are sharing as well as they can receive simple graphical feedback. 

In these applications, the feedback comments usually appear in a list below or beside the video being shared, separate from the visual context of what the viewer is commenting on. This may cause problems when the person sending the video changes his or her viewpoint. For example, a viewer might send the comment “I really like that picture”, but by the time the comment appears, the view might already have changed from the picture being commented on.


Future social interactions with wearable AR can be extrapolated from current social network interactions where friends share content and interact with others' content on mobile platforms such as Facebook and Instagram. One trend with mobile social networks is live streaming of a view of a user’s surroundings. For example, Facebook live allows a person with a mobile phone to live stream to remote collaborators. Similarly, wearable AR systems have already been developed that enable people to share a view of their surroundings. For example, the Shared Sphere work of \cite{lee2017mixed} allows a user with a wearable AR display to live stream a 360 video of their surroundings to a remote collaborator. 

It is easy to imagine that in the future it will be possible for wearable AR systems to be used to capture and share a 3D view of the user's surroundings with hundreds or thousands of followers on a social network. However, before this becomes commonplace, many exciting research questions should be addressed. For example, would a person be comfortable with sharing a view of their real space with relative strangers? This work aims to explore how wearable AR systems could share a user’s surrounding room environment with social contacts and to measure how comfortable the sharer and the viewer would feel regarding privacy in different interface options. 


%----------------------------------------------------------------------------------------
%	Summary
%----------------------------------------------------------------------------------------

This work aims to layout the space of the AR continuum for social sharing experiences by looking at parameters and options that can be changed in terms of people, objects and the environment to create a shared AR experience. This is still work in progress, but aims to cover all dimensions of the AR continuum.




% \subsection{Sharing for Collaboration}
% \subsection{Sharing for Social}
